    print(f"Batch adjusting shapes for {len(all_data)} epochs...")
    
    n_epochs = len(all_data)
    n_channels = 18
    n_times = expected_length
    
    all_epoch_data = []
    
    # 使用更快的向量化操作
    for i in range(n_epochs):
        data = all_data[i]  # [n_channels_original, n_times_original]
        
        # 通道数调整
        if data.shape[0] != n_channels:
            if data.shape[0] > n_channels:
                data = data[:n_channels, :]
            else:
                padding = np.zeros((n_channels - data.shape[0], data.shape[1]))
                data = np.vstack([data, padding])
        
        # 时间点数调整
        if data.shape[1] != n_times:
            if data.shape[1] > n_times:
                excess = data.shape[1] - n_times
                start_idx = excess // 2
                end_idx = start_idx + n_times
                data = data[:, start_idx:end_idx]
            else:
                padding_needed = n_times - data.shape[1]
                pad_left = padding_needed // 2
                pad_right = padding_needed - pad_left
                data = np.pad(data, ((0, 0), (pad_left, pad_right)),
                            mode='constant', constant_values=0)
        
        tensor = torch.FloatTensor(data).unsqueeze(0)  # [1, 18, n_times]
        all_epoch_data.append(tensor)
        
        # 进度显示
        if (i + 1) % 1000 == 0:
            print(f"  Processed {i + 1}/{n_epochs} epochs")
    
    print(f"Prepared {len(all_epoch_data)} epochs in memory")
